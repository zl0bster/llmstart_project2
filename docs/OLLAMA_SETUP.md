# Настройка Ollama для работы с qwen2.5:7b

## Обзор

Ollama - это инструмент для запуска больших языковых моделей локально. В данном проекте используется модель `qwen2.5:7b` для обработки текстовых отчетов ОТК.

## Установка Ollama

### Windows

1. Скачайте Ollama с официального сайта: https://ollama.com/download
2. Запустите установщик и следуйте инструкциям
3. Ollama будет установлен как сервис и запустится автоматически

### Linux/macOS

```bash
# Установка через официальный скрипт
curl -fsSL https://ollama.com/install.sh | sh

# Или через Homebrew (macOS)
brew install ollama
```

## Запуск Ollama

После установки Ollama должен запуститься автоматически. Если нет, запустите вручную:

```bash
# Запуск сервера Ollama
ollama serve
```

По умолчанию Ollama работает на `http://localhost:11434`.

## Установка модели qwen2.5:7b

```bash
# Скачивание модели (может занять несколько минут)
ollama pull qwen2.5:7b
```

Проверьте, что модель установлена:

```bash
# Список установленных моделей
ollama list
```

Должна появиться строка с `qwen2.5:7b`.

## Настройка проекта

### 1. Создание .env файла

Скопируйте пример конфигурации:

```bash
cp env.example .env
```

### 2. Настройка переменных окружения

Отредактируйте `.env` файл:

```env
# Провайдер для текстового анализа
LLM_PROVIDER=ollama

# Модель для использования
TEXT_MODEL=qwen2.5:7b

# URL Ollama сервера
OLLAMA_BASE_URL=http://localhost:11434

# Дополнительные настройки Ollama
OLLAMA_AUTO_PULL=true
OLLAMA_TIMEOUT_SEC=120
OLLAMA_NUM_PREDICT=2000
OLLAMA_TEMPERATURE=0.1
```

### 3. Установка зависимостей

```bash
pip install -r requirements.txt
```

## Тестирование интеграции

Запустите тестовый скрипт для проверки работы:

```bash
python test_ollama_integration.py
```

Скрипт проверит:
- ✅ Подключение к Ollama
- ✅ Наличие модели deepseek-r1:7b
- ✅ Обработку тестового текста

## Настройки модели

### Температура (OLLAMA_TEMPERATURE)

- `0.0` - максимально детерминированные ответы
- `0.1` - рекомендуемое значение для структурированных данных
- `0.7` - более творческие ответы
- `1.0` - максимальная случайность

### Количество токенов (OLLAMA_NUM_PREDICT)

- `500` - короткие ответы
- `2000` - рекомендуемое значение для отчетов ОТК
- `4000` - длинные ответы

### Таймаут (OLLAMA_TIMEOUT_SEC)

- `60` - быстрые запросы
- `120` - рекомендуемое значение для локальных моделей
- `300` - для медленных систем

## Автоматическое скачивание модели

Если включено `OLLAMA_AUTO_PULL=true`, приложение автоматически скачает модель при первом запуске, если она не найдена локально.

## Устранение неполадок

### Ollama не запускается

```bash
# Проверьте статус сервиса
ollama list

# Перезапустите Ollama
ollama serve
```

### Модель не найдена

```bash
# Проверьте установленные модели
ollama list

# Скачайте модель вручную
ollama pull qwen2.5:7b
```

### Ошибки подключения

1. Убедитесь, что Ollama запущен на порту 11434
2. Проверьте URL в настройках: `OLLAMA_BASE_URL=http://localhost:11434`
3. Проверьте, что порт не заблокирован файрволом

### Медленная работа

1. Увеличьте `OLLAMA_TIMEOUT_SEC` до 300
2. Уменьшите `OLLAMA_NUM_PREDICT` до 1000
3. Проверьте загрузку системы

### Ошибки памяти

Модель qwen2.5:7b требует около 4-6 GB RAM. Убедитесь, что у вас достаточно свободной памяти.

## Альтернативные модели

Если qwen2.5:7b не подходит, можно использовать другие модели:

```bash
# Llama 3.1 8B (требует ~8GB RAM)
ollama pull llama3.1:8b

# Mistral 7B (требует ~7GB RAM)
ollama pull mistral:7b

# Phi-3 3.8B (требует ~4GB RAM)
ollama pull phi3:3.8b
```

Затем измените в `.env`:

```env
TEXT_MODEL=llama3.1:8b  # или другая модель
```

## Производительность

### Рекомендуемые системные требования

- **RAM**: 6+ GB для qwen2.5:7b
- **CPU**: 4+ ядра
- **Диск**: 10+ GB свободного места

### Оптимизация

1. Закройте ненужные приложения для освобождения RAM
2. Используйте SSD для хранения моделей
3. Настройте `OLLAMA_NUM_THREADS` для использования всех ядер CPU

```bash
# Установка переменной окружения для количества потоков
export OLLAMA_NUM_THREADS=4
ollama serve
```

## Мониторинг

### Проверка статуса Ollama

```bash
# Список моделей
ollama list

# Информация о модели
ollama show qwen2.5:7b

# Тестовый запрос
ollama run qwen2.5:7b "Привет, как дела?"
```

### Логи приложения

Логи работы с Ollama сохраняются в `logs/llm.log` и содержат:
- Время запроса и ответа
- Размер входного и выходного текста
- Ошибки API

## Обновление модели

```bash
# Обновление модели до последней версии
ollama pull qwen2.5:7b

# Удаление старой версии
ollama rm qwen2.5:7b
```

## Безопасность

- Ollama работает только локально по умолчанию
- Данные не передаются в интернет
- Все обработка происходит на вашем компьютере
- Модели скачиваются из официального реестра Ollama
